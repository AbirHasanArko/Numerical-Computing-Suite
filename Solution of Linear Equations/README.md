# Solution of Linear Equations

[![C++](https://img.shields.io/badge/Language-C++-00599C?style=for-the-badge&logo=cplusplus)](https://isocpp.org/)
[![Numerical Methods](https://img.shields.io/badge/Topic-Numerical%20Methods-FF6B6B?style=for-the-badge)](https://github.com/AbirHasanArko/Numerical-Computing-Suite)

## ğŸ“‘ Table of Contents

- [Introduction](#-introduction)
- [Overview of Solution Methods](#-overview-of-solution-methods)
- [Mathematical Foundation](#-mathematical-foundation)
  - [What are Linear Equations?](#what-are-linear-equations)
  - [Matrix Representation](#matrix-representation)
  - [Types of Solutions](#types-of-solutions)
  - [Fundamental Concepts](#fundamental-concepts)
- [Solution Methods](#-solution-methods)
  - [1. Gauss Elimination Method](#1-gauss-elimination-method)
  - [2. Gauss-Jordan Elimination Method](#2-gauss-jordan-elimination-method)
  - [3. LU Decomposition Method](#3-lu-decomposition-method)
- [Method Comparison](#-method-comparison)
- [Applications](#-applications)
- [Implementation Structure](#-implementation-structure)
- [Getting Started](#-getting-started)
- [References](#-references)
- [Author](#-author)

---

## ğŸ“– Introduction

This section of the **Numerical Computing Suite** focuses on solving **systems of linear equations** using various numerical methods. A system of linear equations is a collection of linear equations involving the same set of variables, and finding solutions to these systems is fundamental to many areas of mathematics, science, and engineering.

Linear systems appear in countless applications: 
- **Engineering**: Circuit analysis, structural analysis, control systems
- **Physics**: Equilibrium problems, quantum mechanics
- **Economics**: Input-output models, optimization
- **Computer Graphics**:  Transformations, rendering
- **Data Science**: Linear regression, machine learning algorithms

This collection provides three powerful methods for solving linear systems, each with its own advantages and use cases.  All implementations include detailed step-by-step visualization, solution type detection, and comprehensive error handling. 

---

## ğŸ” Overview of Solution Methods

This repository implements three classical numerical methods:  

| Method | Best Used For | Complexity | Key Feature |
|--------|--------------|------------|-------------|
| **Gauss Elimination** | Single solution, general systems | O(nÂ³) | Forward elimination + back substitution |
| **Gauss-Jordan Elimination** | Finding inverse, reduced form | O(nÂ³) | Complete diagonal reduction |
| **LU Decomposition** | Multiple systems, repeated solving | O(nÂ³) | Matrix factorization |

---

## ğŸ“ Mathematical Foundation

### What are Linear Equations?

A **linear equation** is an algebraic equation where each term is either a constant or the product of a constant and a single variable. The general form of a linear equation in *n* variables is:

```
aâ‚xâ‚ + aâ‚‚xâ‚‚ + aâ‚ƒxâ‚ƒ + ...  + aâ‚™xâ‚™ = b
```

Where:
- `aâ‚, aâ‚‚, ..., aâ‚™` are coefficients (constants)
- `xâ‚, xâ‚‚, ..., xâ‚™` are variables (unknowns)
- `b` is a constant term

**Properties of Linear Equations:**
- Each variable appears only to the first power
- Variables are not multiplied together (no xâ‚xâ‚‚ terms)
- No variables appear in denominators
- No transcendental functions (sin, cos, exp, log, etc.)

### Matrix Representation

A system of *n* linear equations with *n* unknowns can be represented in matrix form:

```
Ax = b
```

Where:
- **A** is an *n Ã— n* coefficient matrix
- **x** is an *n Ã— 1* solution vector (unknowns)
- **b** is an *n Ã— 1* constant vector (right-hand side)

**Example**:  The system
```
2x + y - z = 8
-3x - y + 2z = -11
-2x + y + 2z = -3
```

Can be written as:
```
â”Œ          â”   â”Œ   â”   â”Œ    â”
â”‚  2  1 -1 â”‚   â”‚ x â”‚   â”‚  8 â”‚
â”‚ -3 -1  2 â”‚ Ã— â”‚ y â”‚ = â”‚-11 â”‚
â”‚ -2  1  2 â”‚   â”‚ z â”‚   â”‚ -3 â”‚
â””          â”˜   â””   â”˜   â””    â”˜
```

**Augmented Matrix Representation**:  `[A|b]`
```
â”Œ              â”
â”‚  2  1 -1 | 8 â”‚
â”‚ -3 -1  2 |-11â”‚
â”‚ -2  1  2 |-3 â”‚
â””              â”˜
```

### Types of Solutions

A system of linear equations can have:  

#### 1. **Unique Solution** ğŸ¯

**Definition**:  Exactly one set of values satisfies all equations simultaneously.

**Conditions**:
- The coefficient matrix **A** is non-singular (det(A) â‰  0)
- rank(A) = rank([A|b]) = n
- All rows are linearly independent

**Geometric Interpretation**:
- In 2D: Two non-parallel lines intersect at one point
- In 3D:  Three non-parallel planes intersect at one point
- In nD:  Hyperplanes intersect at exactly one point

**Example**:
```
x + y = 3
x - y = 1
```
Solution: x = 2, y = 1 (lines intersect at (2, 1))

#### 2. **No Solution** âŒ

**Definition**: No set of values satisfies all equations simultaneously. 

**Conditions**:
- The system is **inconsistent**
- rank(A) < rank([A|b])
- At least one equation contradicts the others

**Geometric Interpretation**:
- In 2D:  Parallel lines that never meet
- In 3D:  Planes that don't have a common intersection point
- System has contradictory constraints

**Example**:
```
x + y = 2
x + y = 5
```
No solution:  parallel lines (same slope, different intercepts)

#### 3. **Infinite Solutions** âˆ

**Definition**:  Infinitely many sets of values satisfy the equations.

**Conditions**: 
- The system is **consistent but underdetermined**
- rank(A) = rank([A|b]) < n
- Some equations are linearly dependent (redundant)

**Geometric Interpretation**:
- In 2D: Coincident lines (same line)
- In 3D: Planes intersect along a line or coincide
- System has free variables (degrees of freedom)

**Example**:
```
x + y = 2
2x + 2y = 4
```
Infinite solutions: y = 2 - x (any point on the line)

### Fundamental Concepts

#### **Rank of a Matrix**

The **rank** is the maximum number of linearly independent rows (or columns) in a matrix. 

**Properties**:
- rank(A) â‰¤ min(m, n) for an mÃ—n matrix
- rank(A) = n means A has full rank (non-singular for square matrices)
- rank(A) < n means A is singular (det(A) = 0)

**Calculation**:  After row reduction, count non-zero rows.

#### **Row Echelon Form (REF)**

A matrix is in row echelon form if:
1. All zero rows are at the bottom
2. The leading entry (pivot) of each non-zero row is to the right of the pivot in the row above
3. All entries below a pivot are zero

**Example**:
```
â”Œ         â”
â”‚ 2  1 -1 â”‚  â† pivot at position (1,1)
â”‚ 0  3  2 â”‚  â† pivot at position (2,2)
â”‚ 0  0  5 â”‚  â† pivot at position (3,3)
â””         â”˜
```

#### **Reduced Row Echelon Form (RREF)**

A matrix is in RREF if it's in REF and additionally:
1. All pivots are 1
2. Each pivot is the only non-zero entry in its column

**Example**:
```
â”Œ         â”
â”‚ 1  0  0 â”‚
â”‚ 0  1  0 â”‚
â”‚ 0  0  1 â”‚
â””         â”˜
```

#### **Determinant**

The determinant is a scalar value that encodes information about a square matrix. 

**Properties**:
- det(A) â‰  0 âŸº A is non-singular âŸº unique solution exists
- det(A) = 0 âŸº A is singular âŸº infinite or no solutions
- det(AB) = det(A) Ã— det(B)
- det(Aâ»Â¹) = 1/det(A)

---

## ğŸ› ï¸ Solution Methods

### 1. Gauss Elimination Method

[![View Implementation](https://img.shields.io/badge/ğŸ“‚-View%20Implementation-blue?style=for-the-badge)](./Gauss%20Elimination%20Method/)

#### Theory

**Gauss Elimination** (named after Carl Friedrich Gauss, 1777-1855) is a systematic algorithm for solving systems of linear equations.  It's one of the most important and widely-used methods in numerical linear algebra, forming the foundation for many advanced techniques.

#### Historical Context

Though named after Gauss, the method was known to ancient Chinese mathematicians (documented in "The Nine Chapters on the Mathematical Art" around 150 BC). Gauss popularized it in the West and extended its applications to least squares problems and geodetic calculations.

#### Mathematical Foundation

The method is based on the principle that **elementary row operations** preserve the solution set of a linear system. These operations are:

1. **Row Swapping**: Interchange two rows (Râ‚ â†” Râ‚‚)
2. **Row Scaling**: Multiply a row by a non-zero scalar (Râ‚ â†’ kRâ‚)
3. **Row Addition**: Add a multiple of one row to another (Râ‚‚ â†’ Râ‚‚ + kRâ‚)

**Key Insight**: These operations transform the system into an equivalent (same solution) but simpler form.

#### The Two-Phase Approach

##### **Phase 1: Forward Elimination**

**Goal**: Transform the augmented matrix [A|b] into **upper triangular form** (row echelon form).

**Process**:  For each column k from 1 to n-1:

1. **Pivot Selection** (with partial pivoting):
   - Find the row i â‰¥ k with the largest |aáµ¢â‚–| in column k
   - Swap row k with row i
   - This element aâ‚–â‚– becomes the **pivot**

2. **Elimination Step**:
   - For each row i below the pivot (i = k+1 to n):
     - Compute multiplier:  `máµ¢â‚– = aáµ¢â‚– / aâ‚–â‚–`
     - Update row i: `Row_i = Row_i - máµ¢â‚– Ã— Row_k`
     - This makes aáµ¢â‚– = 0 (eliminates the element below the pivot)

**Mathematical Representation**: 

Starting with:
```
â”Œ                    â”
â”‚ aâ‚â‚  aâ‚â‚‚  aâ‚â‚ƒ | bâ‚ â”‚
â”‚ aâ‚‚â‚  aâ‚‚â‚‚  aâ‚‚â‚ƒ | bâ‚‚ â”‚
â”‚ aâ‚ƒâ‚  aâ‚ƒâ‚‚  aâ‚ƒâ‚ƒ | bâ‚ƒ â”‚
â””                    â”˜
```

After forward elimination:
```
â”Œ                     â”
â”‚ uâ‚â‚  uâ‚â‚‚  uâ‚â‚ƒ | bâ‚' â”‚
â”‚  0   uâ‚‚â‚‚  uâ‚‚â‚ƒ | bâ‚‚' â”‚
â”‚  0    0   uâ‚ƒâ‚ƒ | bâ‚ƒ' â”‚
â””                     â”˜
```

**Example Walkthrough**: 

Initial system:
```
2x + y - z = 8       â”Œ              â”
-3x - y + 2z = -11   â”‚  2  1 -1 | 8 â”‚
-2x + y + 2z = -3    â”‚ -3 -1  2 |-11â”‚
                     â”‚ -2  1  2 |-3 â”‚
                     â””              â”˜
```

Step 1: Eliminate x from rows 2 and 3
- mâ‚‚â‚ = -3/2 = -1.5
- mâ‚ƒâ‚ = -2/2 = -1. 0

```
â”Œ                    â”
â”‚  2   1  -1 |  8    â”‚
â”‚  0  0. 5 0.5 |  1  â”‚  (Râ‚‚ - mâ‚‚â‚Ã—Râ‚)
â”‚  0   2   1  |  5   â”‚  (Râ‚ƒ - mâ‚ƒâ‚Ã—Râ‚)
â””                    â”˜
```

Step 2: Eliminate y from row 3
- mâ‚ƒâ‚‚ = 2/0.5 = 4

```
â”Œ                    â”
â”‚  2  1   -1 |  8    â”‚
â”‚  0  0.5 0.5|  1    â”‚
â”‚  0  0   -1 |  1    â”‚  (Râ‚ƒ - mâ‚ƒâ‚‚Ã—Râ‚‚)
â””                    â”˜
```

##### **Phase 2: Back Substitution**

**Goal**:  Solve for variables starting from the last equation and working backwards.

**Process**: 

1. **Last Variable**: From the last row, solve directly
   ```
   xâ‚™ = b'â‚™ / uâ‚™â‚™
   ```

2. **Remaining Variables**: For i = n-1, n-2, ..., 1:
   ```
   xáµ¢ = (b'áµ¢ - Î£â±¼â‚Œáµ¢â‚Šâ‚â¿ uáµ¢â±¼ Ã— xâ±¼) / uáµ¢áµ¢
   ```

**Example (continued)**:

From the upper triangular form:
```
2x + y - z = 8
0.5y + 0.5z = 1
-z = 1
```

Solve backwards:
1. z = -1 (from equation 3)
2. y = (1 - 0.5(-1))/0.5 = 3 (substitute z into equation 2)
3. x = (8 - 3 - (-1))/2 = 2 (substitute y and z into equation 1)

**Solution**: x = 2, y = 3, z = -1 âœ“

#### Partial Pivoting Strategy

**Why Pivoting?**

Consider this system without pivoting:
```
0.0001x + y = 1
x + y = 2
```

If we eliminate using the small pivot 0.0001:
- Multiplier = 1/0.0001 = 10,000
- This magnifies round-off errors dramatically! 

**Partial Pivoting Solution**: 
- Always choose the largest available pivot in the current column
- Swap rows to bring it to the pivot position
- This minimizes error propagation

**Algorithm**: 
```
For each column k: 
    Find i â‰¥ k such that |aáµ¢â‚–| is maximum
    Swap row k with row i
    Proceed with elimination
```

**Benefits**:
- âœ… Reduces round-off error accumulation
- âœ… Avoids division by small numbers
- âœ… Improves numerical stability
- âœ… Often prevents division by zero

**Trade-off**: Adds O(nÂ²) comparisons, but the stability gain is worth it.

#### Solution Type Detection

After forward elimination, analyze the resulting upper triangular matrix:

**Step 1: Calculate Rank**
- Count non-zero rows (rows where at least one coefficient â‰  0)
- This gives rank(A) after elimination

**Step 2: Check Consistency**
- If any row has form [0 0 ... 0 | c] where c â‰  0:
  - This means 0 = c, which is impossible
  - System is **inconsistent** â†’ **No Solution**

**Step 3: Compare Ranks**
- If rank(A) = rank([A|b]) = n:
  - System is consistent and determined â†’ **Unique Solution**
- If rank(A) = rank([A|b]) < n:
  - System is consistent but underdetermined â†’ **Infinite Solutions**

**Visual Decision Tree**:
```
After Forward Elimination
        |
        â”œâ”€â†’ Found [0 0 ... 0 | câ‰ 0]?  â†’ YES â†’ No Solution âŒ
        |
        â””â”€â†’ NO â†’ rank = n? â”€â†’ YES â†’ Unique Solution âœ“
                      |
                      â””â”€â†’ NO â†’ Infinite Solutions âˆ
```

#### Complexity Analysis

**Time Complexity**: 

1. **Forward Elimination**:
   - Outer loop: k = 1 to n-1 (n-1 iterations)
   - For each k: 
     - Pivoting: O(n) comparisons
     - For each row i > k: O(n) operations
   - Total:  Î£â‚–â‚Œâ‚â¿â»Â¹ (n-k)Â² â‰ˆ nÂ³/3 operations

2. **Back Substitution**:
   - For variable i: need to compute i-1 multiplications and additions
   - Total:  Î£áµ¢â‚Œâ‚â¿ i = n(n+1)/2 â‰ˆ nÂ²/2 operations

**Overall**:  O(nÂ³) dominated by forward elimination

**Space Complexity**:  O(nÂ²) for storing the augmented matrix

**Exact Operation Counts** (for nÃ—n system):
- **Multiplications/Divisions**: (2nÂ³ + 3nÂ² - 5n)/6 â‰ˆ nÂ³/3
- **Additions/Subtractions**: (nÂ³ - n)/3 â‰ˆ nÂ³/3
- **Comparisons** (with pivoting): n(n-1)/2 â‰ˆ nÂ²/2

#### Numerical Considerations

**Round-off Error Sources**:
1. **Subtractive Cancellation**: When subtracting nearly equal numbers
2. **Small Pivots**: Division by small numbers amplifies errors
3. **Error Propagation**: Errors in early steps affect later computations

**Mitigation Strategies**:
- Use **partial pivoting** (or complete pivoting for extreme cases)
- Use **double precision** floating-point arithmetic
- **Equilibration**:  Scale rows/columns to have similar magnitudes
- **Iterative refinement**: Use solution to improve accuracy

#### Advantages & Limitations

**Advantages**:  
âœ… Straightforward and intuitive algorithm  
âœ… Efficient for single right-hand side  
âœ… Foundation for many other methods  
âœ… Works for any non-singular square system  
âœ… Easy to implement and debug  
âœ… Numerical stability with pivoting  

**Limitations**:  
âŒ Not efficient for multiple right-hand sides (must repeat for each)  
âŒ Requires O(nÂ²) storage  
âŒ Sensitive to round-off errors without pivoting  
âŒ Inefficient for sparse matrices (many zeros)  
âŒ Cannot exploit special matrix structures  

---

### 2. Gauss-Jordan Elimination Method

[![View Implementation](https://img.shields.io/badge/ğŸ“‚-View%20Implementation-green?style=for-the-badge)](./Gauss-Jordan%20Elimination%20Method/)

#### Theory

**Gauss-Jordan Elimination** is an extension of Gaussian elimination that reduces the augmented matrix to **reduced row echelon form (RREF)** rather than just row echelon form.  Named after Carl Friedrich Gauss and Wilhelm Jordan (1842-1899), this method produces the solution directly without requiring back substitution.

#### Historical Context

Wilhelm Jordan, a German geodesist, popularized this variant in his 1888 handbook on geodesy. While Gauss elimination stops at triangular form, Jordan's method continues the elimination process to achieve complete diagonal reduction, making it particularly useful for matrix inversion.

#### Mathematical Foundation

The method extends the three elementary row operations to achieve a more complete reduction:

**Goal**: Transform [A|b] into [I|x], where: 
- **I** is the identity matrix
- **x** is the solution vector

**Key Difference from Gauss Elimination**: 
- Gauss:  Eliminate only **below** each pivot
- Gauss-Jordan: Eliminate **both above and below** each pivot

#### The Complete Reduction Process

##### **Phase 1: Forward Pass**

Similar to Gauss elimination, but with an additional step: 

For each column k from 1 to n: 

1. **Partial Pivoting**:
   - Find row i â‰¥ k with maximum |aáµ¢â‚–|
   - Swap row k with row i

2. **Pivot Normalization**:
   - Divide entire row k by aâ‚–â‚– to make pivot = 1
   - `Row_k = Row_k / aâ‚–â‚–`

3. **Complete Column Elimination**:
   - For **ALL** rows i â‰  k (not just i > k):
     - Compute multiplier: `máµ¢â‚– = aáµ¢â‚–`
     - Update:  `Row_i = Row_i - máµ¢â‚– Ã— Row_k`
   - This makes all elements in column k (except the pivot) equal to zero

**Mathematical Representation**: 

Starting augmented matrix:
```
â”Œ                    â”
â”‚ aâ‚â‚  aâ‚â‚‚  aâ‚â‚ƒ | bâ‚ â”‚
â”‚ aâ‚‚â‚  aâ‚‚â‚‚  aâ‚‚â‚ƒ | bâ‚‚ â”‚
â”‚ aâ‚ƒâ‚  aâ‚ƒâ‚‚  aâ‚ƒâ‚ƒ | bâ‚ƒ â”‚
â””                    â”˜
```

After Gauss-Jordan elimination:
```
â”Œ                      â”
â”‚  1    0    0  | xâ‚   â”‚
â”‚  0    1    0  | xâ‚‚   â”‚
â”‚  0    0    1  | xâ‚ƒ   â”‚
â””                      â”˜
```

The solution is **directly visible**:  xâ‚, xâ‚‚, xâ‚ƒ are in the last column! 

#### Detailed Example Walkthrough

**Initial System**: 
```
2x + y - z = 8
-3x - y + 2z = -11
-2x + y + 2z = -3
```

**Augmented Matrix**:
```
â”Œ              â”
â”‚  2  1 -1 | 8 â”‚
â”‚ -3 -1  2 |-11â”‚
â”‚ -2  1  2 |-3 â”‚
â””              â”˜
```

**Step 1: Process column 1**

a) Pivot is at (1,1): pivot = 2 (or swap if needed)

b) Normalize row 1 (divide by 2):
```
â”Œ                 â”
â”‚  1  0.5 -0.5|4  â”‚  (Râ‚/2)
â”‚ -3  -1    2 |-11â”‚
â”‚ -2   1    2 |-3 â”‚
â””                 â”˜
```

c) Eliminate column 1 in rows 2 and 3:
```
â”Œ                   â”
â”‚  1  0.5 -0.5  | 4 â”‚
â”‚  0  0. 5  0.5 | 1 â”‚  (Râ‚‚ + 3Râ‚)
â”‚  0  2    1    | 5 â”‚  (Râ‚ƒ + 2Râ‚)
â””                   â”˜
```

**Step 2: Process column 2**

a) Pivot is at (2,2): pivot = 0. 5

b) Normalize row 2 (divide by 0.5):
```
â”Œ                   â”
â”‚  1  0.5 -0.5 | 4  â”‚
â”‚  0  1    1   | 2  â”‚  (Râ‚‚/0.5)
â”‚  0  2    1   | 5  â”‚
â””                   â”˜
```

c) Eliminate column 2 in rows 1 and 3:
```
â”Œ                   â”
â”‚  1  0   -1   | 3  â”‚  (Râ‚ - 0.5Râ‚‚)
â”‚  0  1    1   | 2  â”‚
â”‚  0  0   -1   | 1  â”‚  (Râ‚ƒ - 2Râ‚‚)
â””                   â”˜
```

**Step 3: Process column 3**

a) Pivot is at (3,3): pivot = -1

b) Normalize row 3 (divide by -1):
```
â”Œ                   â”
â”‚  1  0  -1   | 3   â”‚
â”‚  0  1   1   | 2   â”‚
â”‚  0  0   1   |-1   â”‚  (Râ‚ƒ/-1)
â””                   â”˜
```

c) Eliminate column 3 in rows 1 and 2:
```
â”Œ                   â”
â”‚  1  0   0   | 2   â”‚  (Râ‚ + Râ‚ƒ)
â”‚  0  1   0   | 3   â”‚  (Râ‚‚ - Râ‚ƒ)
â”‚  0  0   1   |-1   â”‚
â””                   â”˜
```

**Final Result**:  [I|x] form achieved!

**Solution reads directly**: x = 2, y = 3, z = -1 âœ“

#### Key Advantages of RREF

**1. Direct Solution Reading**:
- No back substitution needed
- Solution is immediately visible in the last column
- Less prone to calculation errors in the final step

**2. Matrix Inversion**:
To find Aâ»Â¹, augment A with identity:   [A|I]
Apply Gauss-Jordan:  Result is [I|Aâ»Â¹]

**Example**: Find inverse of 2Ã—2 matrix
```
[2 1|1 0]     [1 0. 5|0.5   0]     [1 0|0.5  -0.5]
[1 3|0 1]  â†’  [1   3|  0   1]  â†’  [0 1|-0.2  0.4]
```
Inverse is: 
```
Aâ»Â¹ = [ 0.5  -0.5]
      [-0.2   0.4]
```

**3. Rank Determination**:
- The number of non-zero rows in RREF equals the rank
- More explicit than row echelon form

**4. Solution Space Visualization**:
- For infinite solutions, free variables are immediately identifiable
- Parametric solution form is easier to construct

#### Comparison with Standard Gauss Elimination

| Aspect | Gauss Elimination | Gauss-Jordan |
|--------|------------------|--------------|
| **Final Form** | Upper triangular | Diagonal (Identity) |
| **Pivots** | Can be any non-zero | Always 1 |
| **Elimination** | Below pivot only | Above & below pivot |
| **Back Substitution** | Required | Not needed |
| **Operations** | ~nÂ³/3 | ~nÂ³/2 |
| **Best For** | Single solution | Matrix inverse |
| **Intuitive** | Moderate | Very intuitive |

#### Algorithm Complexity

**Time Complexity**: 

1. **Forward Pass with Elimination**:
   - For each column k (n iterations):
     - Normalization: O(n) operations
     - Elimination in (n-1) rows: O(nÂ²) operations
   - Total: n Ã— nÂ² = nÂ³ operations

**Exact Count**: ~nÂ³/2 operations (about 50% more than Gauss)

**Space Complexity**: O(nÂ²) for the augmented matrix

**Why More Operations?**
- Gauss:  Eliminates only below pivot â†’ triangular work
- Gauss-Jordan:  Eliminates above and below â†’ rectangular work

**Operation Breakdown**:
- **Multiplications/Divisions**: ~nÂ³/2
- **Additions/Subtractions**: ~nÂ³/2
- **Total**: ~nÂ³ (approximately 1.5 times Gauss elimination)

#### Numerical Stability

**Stability Considerations**: 

1. **Pivoting is Essential**:
   - Without pivoting, method can be highly unstable
   - Partial pivoting is standard practice
   - Complete pivoting rarely needed but possible

2. **Error Propagation**:
   - More elimination steps â†’ more opportunities for error
   - Errors in early steps affect more later steps
   - Partial pivoting mitigates this significantly

3. **Conditioning**:
   - Method's accuracy depends on matrix condition number Îº(A)
   - Well-conditioned:  Îº(A) â‰ˆ 1 â†’ reliable results
   - Ill-conditioned: Îº(A) >> 1 â†’ potential accuracy loss

**Condition Number**:
```
Îº(A) = ||A|| Ã— ||Aâ»Â¹||
```

- Îº(A) = 1: Perfect conditioning (orthogonal matrices)
- Îº(A) < 10Â³: Well-conditioned
- Îº(A) > 10â¶: Ill-conditioned (problematic)

#### Special Applications

**1. Finding Matrix Inverse**:
```
Start:  [A|I]
End:    [I|Aâ»Â¹]
```

**2. Solving Multiple Systems**: 
Augment with multiple b vectors:  [A|bâ‚ bâ‚‚ ... bâ‚–]
Result: [I|xâ‚ xâ‚‚ ...  xâ‚–]

**3. Rank Computation**:
The number of pivots (non-zero rows in RREF) = rank(A)

**4. Basis Finding**: 
Pivot columns in original matrix form a basis for column space

**5. Null Space**:
Free variables in RREF reveal null space basis vectors

#### When to Use Gauss-Jordan

**Optimal Scenarios**:  
âœ… Finding matrix inverse  
âœ… Solving Ax = b for multiple b vectors simultaneously  
âœ… Pedagogical purposes (teaching linear algebra)  
âœ… Explicit RREF needed for analysis  
âœ… When back substitution code is error-prone  
âœ… Small to medium-sized dense matrices  

**Avoid When**:  
âŒ Solving single system (Gauss elimination is faster)  
âŒ Very large matrices (extra operations matter)  
âŒ Sparse matrices (destroys sparsity pattern)  
âŒ When only solution needed (not RREF)  
âŒ Numerical stability is critical (LU better)  

#### Advantages & Limitations

**Advantages**:  
âœ… No back substitution required  
âœ… Solution directly readable  
âœ… Excellent for matrix inversion  
âœ… Very intuitive and teachable  
âœ… Handles multiple right-hand sides efficiently  
âœ… Clear geometric interpretation  

**Limitations**:   
âŒ More operations than standard Gauss (~50% more)  
âŒ Not optimal for single system  
âŒ More floating-point operations â†’ more round-off error  
âŒ Destroys matrix sparsity  
âŒ Inefficient for very large systems  

---

### 3. LU Decomposition Method

[![View Implementation](https://img.shields.io/badge/ğŸ“‚-View%20Implementation-orange?style=for-the-badge)](./LU%20Decomposition/)

#### Theory

**LU Decomposition** (also called LU Factorization) is a matrix factorization method that decomposes a square matrix **A** into the product of a **Lower triangular matrix (L)** and an **Upper triangular matrix (U)**: 

```
A = L Ã— U
```

This powerful technique transforms the problem of solving Ax = b into solving two simpler triangular systems.  It's one of the most important factorizations in numerical linear algebra and forms the basis for many advanced algorithms.

#### Historical Context

The systematic approach to LU decomposition was developed in the 1940s by several mathematicians including mathematician Alan Turing.  The method became practical with the advent of computers, as it allows efficient solutions of linear systems, especially when the matrix A remains fixed but the right-hand side b changes multiple times.

#### Mathematical Foundation

**Core Principle**: Instead of solving Ax = b directly, factor A first: 

```
A = LU
Ax = b  becomes  LUx = b
```

**Two-Step Solution**:
1. Solve **Ly = b** for y (forward substitution)
2. Solve **Ux = y** for x (back substitution)

**Why This Works**:
- Triangular systems are easy to solve:  O(nÂ²) time
- Factorization is done once: O(nÂ³) time
- For k different b vectors:  O(nÂ³ + knÂ²) instead of O(knÂ³)
- **Massive savings** when k > 1! 

#### Types of LU Decomposition

There are several variants of LU decomposition based on how L and U are defined:

##### **1. Doolittle's Method** (Used in our implementation)

**Definition**:
- L has **1's on the diagonal**
- U has **computed values on the diagonal**

**Form**:
```
A = L Ã— U

â”Œ             â”   â”Œ           â”   â”Œ             â”
â”‚ aâ‚â‚ aâ‚â‚‚ aâ‚â‚ƒ â”‚   â”‚ 1   0   0 â”‚   â”‚ uâ‚â‚ uâ‚â‚‚ uâ‚â‚ƒ â”‚
â”‚ aâ‚‚â‚ aâ‚‚â‚‚ aâ‚‚â‚ƒ â”‚ = â”‚ lâ‚‚â‚ 1   0 â”‚ Ã— â”‚ 0   uâ‚‚â‚‚ uâ‚‚â‚ƒ â”‚
â”‚ aâ‚ƒâ‚ aâ‚ƒâ‚‚ aâ‚ƒâ‚ƒ â”‚   â”‚ lâ‚ƒâ‚ lâ‚ƒâ‚‚ 1 â”‚   â”‚ 0    0  uâ‚ƒâ‚ƒ â”‚
â””             â”˜   â””           â”˜   â””             â”˜
```

**Characteristics**:
- Most commonly used
- Natural extension of Gaussian elimination
- U matrix is exactly what you get from forward elimination

##### **2. Crout's Method**

**Definition**:
- L has **computed values on the diagonal**
- U has **1's on the diagonal**

**Form**:
```
â”Œ            â”   â”Œ             â”
â”‚ lâ‚â‚  0  0  â”‚   â”‚  1  uâ‚â‚‚ uâ‚â‚ƒ â”‚
â”‚ lâ‚‚â‚ lâ‚‚â‚‚ 0  â”‚ Ã— â”‚  0   1  uâ‚‚â‚ƒ â”‚
â”‚ lâ‚ƒâ‚ lâ‚ƒâ‚‚ lâ‚ƒâ‚ƒâ”‚   â”‚  0   0   1  â”‚
â””            â”˜   â””             â”˜
```

##### **3. Cholesky Decomposition** (For symmetric positive-definite matrices)

**Definition**: A = L Ã— Láµ€
- Special case where U = Láµ€
- Only works for symmetric positive-definite matrices
- Requires ~nÂ³/6 operations (half of standard LU)

#### Detailed Algorithm:  Doolittle's Method

##### **Step 1: Decomposition Process**

The decomposition proceeds column by column and row by row:

**For each column k from 1 to n:**

**Part A:  Compute U (Upper Triangular)**

For each element uáµ¢â‚– where i â‰¤ k: 
```
uáµ¢â‚– = aáµ¢â‚– - Î£â±¼â‚Œâ‚â±â»Â¹ láµ¢â±¼ Ã— uâ±¼â‚–
```

**Part B: Compute L (Lower Triangular)**

For each element lâ‚–áµ¢ where i > k:
```
lâ‚–áµ¢ = (aâ‚–áµ¢ - Î£â±¼â‚Œâ‚áµâ»Â¹ láµ¢â±¼ Ã— uâ±¼â‚–) / uâ‚–â‚–
```

**Diagonal of L**: lâ‚–â‚– = 1 (by definition in Doolittle's method)

##### **Complete Algorithm** (Doolittle's Method)

```
Initialize L and U as nÃ—n zero matrices
Set diagonal of L to 1

For i = 0 to n-1:
    
    // Compute U[i][k] for k = i to n-1
    For k = i to n-1:
        sum = 0
        For j = 0 to i-1:
            sum += L[i][j] Ã— U[j][k]
        U[i][k] = A[i][k] - sum
    
    // Check for zero pivot
    If |U[i][i]| < Îµ:
        Matrix is singular (decomposition fails)
        Return error
    
    // Compute L[k][i] for k = i+1 to n-1
    For k = i+1 to n-1:
        sum = 0
        For j = 0 to i-1:
            sum += L[k][j] Ã— U[j][i]
        L[k][i] = (A[k][i] - sum) / U[i][i]
```

#### Detailed Example:  3Ã—3 Matrix

**Given System**:
```
2x + y - z = 8
-3x - y + 2z = -11
-2x + y + 2z = -3
```

**Matrix A**:
```
A = â”Œ           â”
    â”‚  2  1 -1  â”‚
    â”‚ -3 -1  2  â”‚
    â”‚ -2  1  2  â”‚
    â””           â”˜
```

**Step-by-Step Decomposition**: 

**Iteration 1** (i = 0):

Compute U[0][k] for k = 0, 1, 2:
```
U[0][0] = A[0][0] = 2
U[0][1] = A[0][1] = 1
U[0][2] = A[0][2] = -1
```

Compute L[k][0] for k = 1, 2:
```
L[1][0] = A[1][0] / U[0][0] = -3 / 2 = -1.5
L[2][0] = A[2][0] / U[0][0] = -2 / 2 = -1.0
```

**Current State**:
```
L = â”Œ           â”      U = â”Œ          â”
    â”‚  1  0  0  â”‚          â”‚ 2  1 -1  â”‚
    â”‚-1.5 1  0  â”‚          â”‚ 0  0  0  â”‚
    â”‚-1.0 0  1  â”‚          â”‚ 0  0  0  â”‚
    â””           â”˜          â””          â”˜
```

**Iteration 2** (i = 1):

Compute U[1][k] for k = 1, 2:
```
U[1][1] = A[1][1] - L[1][0]Ã—U[0][1]
        = -1 - (-1.5)Ã—1
        = -1 + 1.5 = 0.5

U[1][2] = A[1][2] - L[1][0]Ã—U[0][2]
        = 2 - (-1.5)Ã—(-1)
        = 2 - 1.5 = 0.5
```

Compute L[2][1]: 
```
L[2][1] = (A[2][1] - L[2][0]Ã—U[0][1]) / U[1][1]
        = (1 - (-1.0)Ã—1) / 0.5
        = (1 + 1) / 0.5 = 4. 0
```

**Current State**:
```
L = â”Œ           â”      U = â”Œ            â”
    â”‚  1   0  0 â”‚          â”‚ 2   1  -1  â”‚
    â”‚-1.5  1  0 â”‚          â”‚ 0  0.5 0.5 â”‚
    â”‚-1.0  4  1 â”‚          â”‚ 0   0   0  â”‚
    â””           â”˜          â””            â”˜
```

**Iteration 3** (i = 2):

Compute U[2][2]: 
```
U[2][2] = A[2][2] - L[2][0]Ã—U[0][2] - L[2][1]Ã—U[1][2]
        = 2 - (-1.0)Ã—(-1) - 4.0Ã—0.5
        = 2 - 1 - 2 = -1.0
```

**Final Decomposition**:
```
L = â”Œ           â”      U = â”Œ            â”
    â”‚  1   0  0 â”‚          â”‚ 2   1  -1  â”‚
    â”‚-1.5  1  0 â”‚          â”‚ 0  0.5 0.5 â”‚
    â”‚-1.0  4  1 â”‚          â”‚ 0   0 -1.0 â”‚
    â””           â”˜          â””            â”˜
```

**Verification**:  Compute L Ã— U
```
(L Ã— U)[0][0] = 1Ã—2 + 0Ã—0 + 0Ã—0 = 2 âœ“
(L Ã— U)[1][0] = -1.5Ã—2 + 1Ã—0 + 0Ã—0 = -3 âœ“
(L Ã— U)[1][1] = -1.5Ã—1 + 1Ã—0.5 + 0Ã—0 = -1 âœ“
...  (all entries match A)
```

##### **Step 2: Forward Substitution (Solve Ly = b)**

Given:  Ly = b, find y

**Process**:  Solve from top to bottom
```
For i = 0 to n-1:
    y[i] = b[i]
    For j = 0 to i-1:
        y[i] -= L[i][j] Ã— y[j]
    // Note: No division needed since L[i][i] = 1
```

**Example** (continuing from above, b = [8, -11, -3]):

```
y[0] = 8

y[1] = -11 - L[1][0]Ã—y[0]
     = -11 - (-1.5)Ã—8
     = -11 + 12 = 1

y[2] = -3 - L[2][0]Ã—y[0] - L[2][1]Ã—y[1]
     = -3 - (-1.0)Ã—8 - 4.0Ã—1
     = -3 + 8 - 4 = 1
```

**Result**: y = [8, 1, 1]

##### **Step 3: Back Substitution (Solve Ux = y)**

Given: Ux = y, find x

**Process**: Solve from bottom to top
```
For i = n-1 down to 0:
    x[i] = y[i]
    For j = i+1 to n-1:
        x[i] -= U[i][j] Ã— x[j]
    x[i] /= U[i][i]
```

**Example** (y = [8, 1, 1]):

```
x[2] = 1 / U[2][2]
     = 1 / (-1.0) = -1

x[1] = (1 - U[1][2]Ã—x[2]) / U[1][1]
     = (1 - 0.5Ã—(-1)) / 0.5
     = (1 + 0.5) / 0.5 = 3

x[0] = (8 - U[0][1]Ã—x[1] - U[0][2]Ã—x[2]) / U[0][0]
     = (8 - 1Ã—3 - (-1)Ã—(-1)) / 2
     = (8 - 3 - 1) / 2 = 2
```

**Final Solution**: x = [2, 3, -1] âœ“

#### The Power of LU:  Multiple Right-Hand Sides

**Scenario**: Solve Ax = b for different b vectors

**Without LU** (using Gauss elimination k times):
- Cost: k Ã— (nÂ³/3) â‰ˆ knÂ³/3

**With LU**:
1.  Decompose A once: nÂ³/3 operations
2. For each b:  Forward + Back substitution:  2nÂ² operations
- Total cost: nÂ³/3 + k(2nÂ²) â‰ˆ nÂ³/3 + 2knÂ²

**Comparison** (for n = 1000):
- k = 1: LU â‰ˆ same as Gauss
- k = 10: LU â‰ˆ 90% faster! 
- k = 100: LU â‰ˆ 99% faster!

**Example Application**: Circuit analysis with varying input voltages
- Circuit topology (matrix A) stays the same
- Input voltages (vector b) change
- Decompose once, solve many times efficiently! 

#### Connection to Gaussian Elimination

**Key Insight**: LU decomposition is essentially Gaussian elimination in disguise! 

**The Connection**:
- U is exactly the upper triangular matrix from forward elimination
- L encodes all the multipliers used during elimination

**Gauss Elimination**:
```
mâ‚‚â‚ = aâ‚‚â‚/aâ‚â‚
Row2 = Row2 - mâ‚‚â‚Ã—Row1
```

**LU Decomposition**:
```
L[2][1] = mâ‚‚â‚  (stores the multiplier)
U is the result after elimination
```

**Why Separate Them?**:
- LU explicitly stores the factorization
- Can reuse for different b vectors
- Enables additional operations (determinants, inverses)

#### Determinant Calculation

One beautiful property of LU decomposition: 

```
det(A) = det(L) Ã— det(U)
       = 1 Ã— det(U)           (since det(L) = 1 in Doolittle's)
       = âˆáµ¢â‚Œâ‚â¿ U[i][i]        (product of U's diagonal)
```

**Example**:
```
U = â”Œ            â”
    â”‚ 2   1  -1  â”‚
    â”‚ 0  0.5 0.5 â”‚
    â”‚ 0   0  -1  â”‚
    â””            â”˜

det(A) = 2 Ã— 0.5 Ã— (-1) = -1
```

**Advantages**:
- Computing determinant is now O(n) after decomposition! 
- Without LU:  determinant computation is O(nÂ³)

#### Pivoting in LU Decomposition

**The Problem**: A might need row swaps for stability

**Solution**: PA = LU (LU with Partial Pivoting)
- P is a **permutation matrix** (encodes row swaps)
- Solve:  PAx = Pb becomes LUx = Pb

**Algorithm Modification**:
```
At each step i:
    Find row k â‰¥ i with maximum |A[k][i]|
    Swap rows i and k in A
    Update permutation matrix P
    Continue with standard LU
```

**Permutation Matrix Example**:
```
P = â”Œ       â”    Represents:  swap rows 1 and 2
    â”‚ 0 1 0 â”‚
    â”‚ 1 0 0 â”‚
    â”‚ 0 0 1 â”‚
    â””       â”˜
```

#### Complexity Analysis

**Time Complexity**: 

1. **LU Decomposition** (one-time cost):
   - Exact operations: (2nÂ³ - 3nÂ² + n)/6 â‰ˆ nÂ³/3
   - Dominant term: O(nÂ³)

2. **Forward Substitution** (per solve):
   - Operations: nÂ² - n â‰ˆ nÂ²
   - Complexity: O(nÂ²)

3. **Back Substitution** (per solve):
   - Operations: nÂ² â‰ˆ nÂ²
   - Complexity: O(nÂ²)

**Total for k Right-Hand Sides**:
- Decomposition: ~nÂ³/3
- k Solutions: ~2knÂ²
- **Overall**:  O(nÂ³ + knÂ²)

**Comparison with Gauss Elimination** (k systems):
- Gauss: O(knÂ³/3)
- LU: O(nÂ³/3 + 2knÂ²)
- **Crossover**: LU wins when k â‰¥ 2

**Space Complexity**:
- Store L, U, b, x: O(2nÂ² + 2n) = O(nÂ²)
- Can overwrite A with L and U: O(nÂ²)

#### Numerical Stability and Conditioning

**Stability Factors**: 

1. **Growth Factor**:
   - Measures how large elements can become during decomposition
   - Without pivoting: can grow exponentially
   - With partial pivoting: usually bounded by 2â¿â»Â¹ (rarely achieved)

2. **Condition Number Sensitivity**:
   - For ill-conditioned matrices (Îº(A) >> 1):
     - Solution accuracy degrades
     - Pivoting essential
     - Consider iterative refinement

3. **Partial Pivoting Benefits**:
   - Usually sufficient for stability
   - Keeps maximum element in U â‰¤ 2Ã—(max element in A)
   - Industry standard approach

**Error Bound**:
```
||x_computed - x_exact|| / ||x_exact|| â‰ˆ Îº(A) Ã— machine_epsilon
```

#### Special Cases and Variants

**1. Tridiagonal Matrices**:
- Special LU algorithm:  O(n) time instead of O(nÂ³)
- Common in differential equations

**2. Band Matrices**:
- Bandwidth b:  only O(nbÂ²) operations
- Preserves band structure in L and U

**3. Sparse Matrices**:
- Fill-in problem: zeros can become non-zero
- Reordering strategies minimize fill-in
- Specialized sparse LU algorithms

**4. Positive Definite Matrices**: 
- Use Cholesky:  A = LLáµ€
- Half the operations of LU
- More stable

#### When to Use LU Decomposition

**Optimal Scenarios**:  
âœ… **Solving multiple systems** with same A (k â‰¥ 2)  
âœ… **Computing determinants** efficiently  
âœ… **Matrix inversion** (solve Ax = eáµ¢ for each unit vector)  
âœ… **Large systems** where reusability matters  
âœ… **Numerical libraries** (LAPACK, BLAS implementations)  
âœ… **Foundation for advanced methods** (iterative refinement)  

**Avoid When**:  
âŒ Single system, small n (Gauss elimination simpler)  
âŒ Sparse matrices without reordering  
âŒ Ill-conditioned systems (use QR or SVD)  
âŒ Memory constrained (stores L and U)  

#### Advantages & Limitations

**Advantages**:  
âœ… **Extreme efficiency** for multiple right-hand sides  
âœ… **One decomposition**, many solutions  
âœ… **Easy determinant** calculation:  O(n) after decomposition  
âœ… **Matrix inversion** natural application  
âœ… **Foundation** for many advanced algorithms  
âœ… **Numerical stability** with pivoting  
âœ… **Explicit factorization** useful for analysis  
âœ… **Parallelizable** to some extent  

**Limitations**:  
âŒ **Higher memory** requirement (store L and U)  
âŒ **Not optimal** for single solution  
âŒ **Requires pivoting** for stability  
âŒ **Fill-in problem** for sparse matrices  
âŒ **Decomposition fails** if matrix is singular  
âŒ **Not best** for ill-conditioned systems  
âŒ **More complex** to implement correctly  

---

## ğŸ“Š Method Comparison

### When to Use Each Method?  

| Scenario | Recommended Method | Reason |
|----------|-------------------|--------|
| Solving one system once | Gauss Elimination | Simplest, least memory, adequate performance |
| Need solution without back substitution | Gauss-Jordan | Direct reading from RREF |
| Multiple systems, same A | **LU Decomposition** | Decompose once, massive savings |
| Finding matrix inverse | Gauss-Jordan or LU | Both efficient, GJ more straightforward |
| Numerical stability critical | LU with pivoting | Best error control |
| Teaching linear algebra | Gauss-Jordan | Most intuitive, clear steps |
| Large sparse matrices | Specialized methods | Standard methods destroy sparsity |
| Ill-conditioned systems | QR or SVD | Better numerical properties |

### Computational Cost Comparison

For solving `Ax = b` where A is nÃ—n:

| Method | Single Solution | k Different b vectors | Memory |
|--------|----------------|---------------------|---------|
| **Gauss Elimination** | ~nÂ³/3 + nÂ² | ~k(nÂ³/3) | O(nÂ²) |
| **Gauss-Jordan** | ~nÂ³/2 + nÂ² | ~k(nÂ³/2) | O(nÂ²) |
| **LU Decomposition** | ~nÂ³/3 + 2nÂ² | ~nÂ³/3 + 2knÂ² | O(2nÂ²) |

**Concrete Example** (n = 1000):

| k | Gauss Ops | LU Ops | LU Speedup |
|---|-----------|---------|------------|
| 1 | 3.3Ã—10â¸ | 3.3Ã—10â¸ | ~1Ã— |
| 10 | 3.3Ã—10â¹ | 3.5Ã—10â¸ | ~9. 4Ã— |
| 100 | 3.3Ã—10Â¹â° | 5.3Ã—10â¸ | ~62Ã— |

**Winner Analysis**:
- **k = 1**:  Gauss Elimination (slightly simpler)
- **k = 2-5**: LU starts winning
- **k > 5**: LU dramatically better

### Operation Count Details

**Forward Elimination/Decomposition**:
```
Gauss:         nÂ³/3 - nÂ²/2 + n/6
Gauss-Jordan:  nÂ³/2
LU Doolittle:  nÂ³/3
```

**Back/Forward Substitution**:
```
Gauss:        nÂ²/2
Gauss-Jordan: 0 (included in elimination)
LU (both):    nÂ²
```

### Stability Ranking

From most to least stable (with proper pivoting):

1. **LU with Partial Pivoting** â­â­â­â­â­
   - Industry standard
   - Excellent stability-performance balance

2. **Gauss with Partial Pivoting** â­â­â­â­
   - Very stable
   - Slight edge to LU for multiple solves

3. **Gauss-Jordan with Pivoting** â­â­â­
   - More operations â†’ more rounding errors
   - Still acceptable for most applications

**Note**: All methods MUST use pivoting for reliable results!

---

## ğŸ¯ Applications

### Direct Applications

#### 1. **Circuit Analysis** âš¡
**Problem**: Find voltages and currents in electrical networks

**Approach**:  Apply Kirchhoff's laws
- Current law:  Î£ currents = 0 at each node
- Voltage law:  Î£ voltages = 0 around each loop
- Results in system of linear equations

**Why LU? **:  Circuit topology stays constant, but inputs vary
- Decompose circuit matrix once
- Solve for different voltage sources quickly

#### 2. **Structural Engineering** ğŸ—ï¸
**Problem**: Analyze forces in trusses, beams, buildings

**Approach**: Equilibrium equations
- Force balance at each node
- Moment balance
- Linear system:  [Stiffness Matrix] Ã— [Displacements] = [Forces]

**Why Gauss?**: Often single load case analysis

#### 3. **Chemical Engineering** âš—ï¸
**Problem**:  Material balance in reactors, distillation columns

**Approach**:  Conservation laws
- Mass balance
- Energy balance
- Component balance

**Why LU?**: Same process, different feed compositions

#### 4. **Economics** ğŸ’°
**Problem**:  Input-output models (Leontief models)

**Approach**: (I - A)x = d
- A: input-output matrix
- x: production levels
- d: final demand

**Why Gauss-Jordan?**: Often need to analyze matrix properties

### Computational Applications

#### 5. **Computer Graphics** ğŸ¨
- **3D Transformations**: Solving for transformation matrices
- **Camera Calibration**: From known points to camera parameters
- **Lighting Calculations**:  Radiosity methods
- **Curve/Surface Fitting**: Interpolation control points

#### 6. **Machine Learning** ğŸ¤–
- **Linear Regression**: Normal equations Xáµ€ Xx = Xáµ€y
- **Principal Component Analysis**: Eigenvalue problems
- **Neural Network Training**: Weight updates
- **Support Vector Machines**: Quadratic programming subproblems

#### 7. **Physics Simulations** âš›ï¸
- **Quantum Mechanics**: SchrÃ¶dinger equation discretization
- **Fluid Dynamics**: Navier-Stokes equations (linearized)
- **Heat Transfer**: Finite difference/element methods
- **Wave Propagation**:  Helmholtz equation

#### 8. **Data Science** ğŸ“Š
- **Least Squares Fitting**: Overdetermined systems
- **Polynomial Interpolation**: Vandermonde systems
- **Statistical Inference**: Normal equations
- **Network Analysis**: Graph Laplacians

### Real-World Example: Power Grid

**Problem**: Calculate power flow in electrical grid

**Setup**:
- 1000 buses (nodes) in the network
- Need to solve for voltage at each bus
- Topology rarely changes, but loads change hourly

**Without LU**: 
- Solve 1000Ã—1000 system 24 times per day
- Cost: 24 Ã— (nÂ³/3) â‰ˆ 8 billion operations/day

**With LU**: 
- Decompose once per topology change (monthly)
- Solve 24 times per day:  24 Ã— (2nÂ²) â‰ˆ 48 million operations/day
- **Speedup:  ~166Ã—** for daily operations! 

---

## ğŸ“ Implementation Structure

Each method folder contains:

```
Method Name/
â”œâ”€â”€ README.md                 # Detailed theory and examples
â”œâ”€â”€ method-name.cpp           # C++ implementation
â”œâ”€â”€ input. txt                # Sample test cases
â””â”€â”€ output.txt                # Expected outputs
```

### Common Features Across All Implementations

**Input/Output**:
- âœ… File-based I/O for batch processing
- âœ… Multiple test cases per file
- âœ… Formatted output with configurable precision

**Visualization**:
- âœ… Step-by-step matrix transformations
- âœ… Intermediate results display (togglable)
- âœ… Final solution verification

**Error Handling**: 
- âœ… Solution type detection (unique/none/infinite)
- âœ… Singular matrix detection
- âœ… Numerical stability checks (epsilon comparisons)
- âœ… Input validation

**Code Quality**:
- âœ… Well-commented code
- âœ… Consistent formatting
- âœ… Readable variable names
- âœ… Modular structure

---

## ğŸš€ Getting Started

### Prerequisites
- **C++ Compiler**: g++, clang++, or MSVC
- **C++ Standard**: C++11 or later
- **Text Editor/IDE**: VS Code, CLion, or any preferred editor
- **Basic Knowledge**: Linear algebra fundamentals

### Compilation

Navigate to any method folder and compile:

```bash
# Standard compilation
g++ -o solver method-name.cpp -std=c++11

# With optimization (recommended)
g++ -o solver method-name.cpp -std=c++17 -O2

# With warnings (for development)
g++ -o solver method-name.cpp -std=c++17 -O2 -Wall -Wextra
```

### Execution

```bash
# Run the compiled program
./solver

# On Windows
solver.exe
```

The program: 
1. Reads input from `input.txt`
2. Processes all test cases
3. Writes results to `output.txt`
4. Displays completion message

### Input Format

```
n                    # Number of equations
aâ‚â‚ aâ‚â‚‚ ... aâ‚â‚™ bâ‚   # First equation coefficients and constant
aâ‚‚â‚ aâ‚‚â‚‚ ... aâ‚‚â‚™ bâ‚‚   # Second equation coefficients and constant
...
aâ‚™â‚ aâ‚™â‚‚ ... aâ‚™â‚™ bâ‚™     # nth equation coefficients and constant

# Multiple test cases can follow
```

**Example**:
```
3
2 1 -1 8
-3 -1 2 -11
-2 1 2 -3

3
1 2 3 6
2 4 6 12
3 6 9 18
```

**Represents**:
```
Test Case 1:
2x + y - z = 8
-3x - y + 2z = -11
-2x + y + 2z = -3

Test Case 2:
x + 2y + 3z = 6
2x + 4y + 6z = 12
3x + 6y + 9z = 18
```

### Output Format

Each test case produces: 
1. **Input system display** (equations in readable form)
2. **Intermediate steps** (matrix transformations)
3. **Solution type** (Unique/None/Infinite)
4. **Final solution** (if unique)
5. **Verification** (Ax = b check)

### Customization Options

**Toggle Intermediate Output**:
```cpp
bool printIntermediate = true;  // Set to false for final results only
```

**Adjust Precision**:
```cpp
fout << fixed << setprecision(4);  // Change 4 to desired decimal places
```

**Modify Zero Threshold**:
```cpp
const double EPSILON = 1e-12;  // Adjust for your numerical requirements
```

---

## ğŸ“š References

- **Numerical Methods for Engineers** by Chapra & Canale
- [Wikipedia: System of Linear Equations](https://en.wikipedia.org/wiki/System_of_linear_equations)

---

## ğŸ‘¨â€ğŸ’» Author

**Abir Hasan Arko**  
[![GitHub](https://img.shields.io/badge/GitHub-AbirHasanArko-181717?style=flat&logo=github)](https://github.com/AbirHasanArko)

---

<div align="center">

**[â¬† Back to Top](#solution-of-linear-equations)**

Part of the [Numerical Computing Suite](https://github.com/AbirHasanArko/Numerical-Computing-Suite)

</div>